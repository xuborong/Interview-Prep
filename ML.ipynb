{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### What is Bayes's Theorem in Machine Learning?\n",
    "\n",
    "---\n",
    "\n",
    "### **Bayes's Theorem in Machine Learning**\n",
    "\n",
    "Bayes's Theorem is a fundamental concept in probability theory and plays a crucial role in machine learning, particularly in the context of probabilistic models and decision-making processes. It provides a way to update the probability of a hypothesis based on new evidence.\n",
    "\n",
    "#### **Bayes's Theorem:**\n",
    "Bayes's Theorem is mathematically expressed as:\n",
    "\n",
    "$$\n",
    "P(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\(P(H|E)\\) is the **posterior probability**, the probability of the hypothesis \\(H\\) given the evidence \\(E\\).\n",
    "- \\(P(E|H)\\) is the **likelihood**, the probability of the evidence \\(E\\) given that the hypothesis \\(H\\) is true.\n",
    "- \\(P(H)\\) is the **prior probability** of the hypothesis \\(H\\) before observing the evidence.\n",
    "- \\(P(E)\\) is the **marginal likelihood** or **evidence**, the probability of observing the evidence \\(E\\) under all possible hypotheses.\n",
    "\n",
    "#### **Application in Machine Learning:**\n",
    "In machine learning, Bayes's Theorem is often used in various probabilistic models, such as Naive Bayes classifiers, Bayesian networks, and in Bayesian inference for updating model parameters.\n",
    "\n",
    "##### **Example - Naive Bayes Classifier:**\n",
    "In a Naive Bayes classifier, the goal is to predict the probability of a class label \\(C\\) given a set of features \\(X_1, X_2, \\dots, X_n\\). According to Bayes's Theorem:\n",
    "\n",
    "$$\n",
    "P(C|X_1, X_2, \\dots, X_n) = \\frac{P(X_1, X_2, \\dots, X_n|C) \\cdot P(C)}{P(X_1, X_2, \\dots, X_n)}\n",
    "$$\n",
    "\n",
    "Given that calculating \\(P(X_1, X_2, \\dots, X_n)\\) can be complex, Naive Bayes assumes that the features are conditionally independent, simplifying the likelihood term to a product of individual probabilities:\n",
    "\n",
    "$$\n",
    "P(C|X_1, X_2, \\dots, X_n) \\propto P(C) \\cdot \\prod_{i=1}^{n} P(X_i|C)\n",
    "$$\n",
    "\n",
    "This simplification allows for efficient computation and is effective in many practical scenarios, despite the strong independence assumption.\n",
    "\n",
    "#### **Why It’s Important:**\n",
    "Bayes's Theorem allows for the incorporation of prior knowledge or beliefs (the prior) and refines these beliefs as new data or evidence becomes available. This is particularly useful in machine learning for:\n",
    "- **Updating Models:** As new data is observed, Bayes's Theorem can update the model's predictions, making it adaptive and robust.\n",
    "- **Handling Uncertainty:** It provides a framework for quantifying uncertainty and making probabilistic predictions, which is essential in decision-making under uncertainty.\n",
    "- **Model Selection:** Bayesian methods can be used to compare models by calculating the posterior probabilities of different models given the data.\n",
    "\n",
    "In summary, Bayes's Theorem is a foundational tool in machine learning that enables probabilistic reasoning and helps to build models that can learn and adapt based on new data."
   ],
   "id": "4d59a010962803dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "**Component Analysis (PCA)** is a widely used dimensionality reduction technique in machine learning and data analysis. It transforms a large set of variables into a smaller one that still contains most of the information in the original set. PCA is particularly useful for dealing with high-dimensional data, where visualizing and processing the data can be challenging.\n",
    "\n",
    "### Key Concepts of PCA:\n",
    "\n",
    "1. **Dimensionality Reduction:**\n",
    "   - PCA reduces the number of variables (features) in your data while preserving as much variance (information) as possible.\n",
    "   - This is achieved by identifying directions, called principal components, along which the variance of the data is maximized.\n",
    "\n",
    "2. **Principal Components:**\n",
    "   - The principal components are new, uncorrelated variables that are linear combinations of the original variables.\n",
    "   - The first principal component captures the largest possible variance in the data. The second principal component captures the second largest variance, and so on.\n",
    "   - The number of principal components is less than or equal to the number of original variables.\n",
    "\n",
    "3. **Variance and Eigenvectors:**\n",
    "   - PCA works by calculating the covariance matrix of the data and then finding its eigenvectors and eigenvalues.\n",
    "   - The eigenvectors correspond to the directions of maximum variance (the principal components), and the eigenvalues indicate the magnitude of the variance in these directions.\n",
    "\n",
    "4. **Projection:**\n",
    "   - The data is projected onto the principal components, which reduces the dimensionality of the dataset.\n",
    "   - By selecting the top few principal components (those with the highest eigenvalues), you can reduce the number of dimensions while retaining most of the data's variability.\n",
    "\n",
    "### How PCA is Used in Machine Learning:\n",
    "\n",
    "1. **Data Preprocessing:**\n",
    "   - PCA is often used as a preprocessing step to reduce the dimensionality of the data before applying other machine learning algorithms, especially when dealing with high-dimensional datasets.\n",
    "   - It helps in reducing noise and computational costs and can improve the performance of certain algorithms.\n",
    "\n",
    "2. **Visualization:**\n",
    "   - PCA is commonly used to visualize high-dimensional data in 2D or 3D. By projecting the data onto the first two or three principal components, you can get a clearer picture of the underlying structure of the data.\n",
    "\n",
    "3. **Feature Extraction:**\n",
    "   - In some cases, the principal components themselves can be used as new features for machine learning models. These new features are often more informative and less correlated than the original features.\n",
    "\n",
    "### Example of PCA in Practice:\n",
    "\n",
    "Imagine you have a dataset with hundreds of features, but you suspect that many of these features are redundant or irrelevant. By applying PCA, you can reduce the number of features to a smaller set of principal components that still capture the most important patterns in the data. This reduced set of features can then be used to train a machine learning model, potentially improving its performance and interpretability.\n",
    "\n",
    "### Limitations of PCA:\n",
    "\n",
    "- **Interpretability:** The principal components are linear combinations of the original features, which can make them difficult to interpret in terms of the original data.\n",
    "- **Linearity:** PCA assumes that the principal components are linear combinations of the original features. It may not perform well if the relationships in the data are highly non-linear.\n",
    "- **Variance:** PCA focuses on maximizing variance, but this does not always correspond to the most important features for a specific machine learning task.\n",
    "\n",
    "In summary, PCA is a powerful technique for dimensionality reduction and feature extraction, making it easier to work with complex datasets and often leading to better-performing models. However, like any technique, it should be used with an understanding of its assumptions and limitations."
   ],
   "id": "9e287d3ace3cc34d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used primarily for classification tasks, but it can also be applied to regression problems (in which case it’s called Support Vector Regression, or SVR). SVM is widely appreciated for its effectiveness in high-dimensional spaces and its ability to create robust classifiers with well-defined decision boundaries.\n",
    "\n",
    "### Key Concepts of Support Vector Machine:\n",
    "\n",
    "1. **Hyperplane:**\n",
    "   - In the context of SVM, a hyperplane is a decision boundary that separates different classes in the feature space.\n",
    "   - For a binary classification problem, SVM tries to find the hyperplane that best separates the data points of the two classes. In higher dimensions, the hyperplane is a generalization of a line (in 2D) or a plane (in 3D).\n",
    "\n",
    "2. **Support Vectors:**\n",
    "   - The support vectors are the data points that are closest to the hyperplane. These points are crucial because they define the position and orientation of the hyperplane.\n",
    "   - The SVM algorithm chooses the hyperplane that maximizes the margin, which is the distance between the hyperplane and the closest support vectors from either class.\n",
    "\n",
    "3. **Margin:**\n",
    "   - The margin is the distance between the hyperplane and the nearest data points from either class. SVM aims to maximize this margin, as a larger margin implies a better generalization ability of the classifier.\n",
    "   - A larger margin reduces the risk of misclassification of new data points.\n",
    "\n",
    "4. **Kernel Trick:**\n",
    "   - SVM can efficiently perform a non-linear classification using the kernel trick. The kernel trick involves mapping the original features into a higher-dimensional space where a linear hyperplane can be used to separate the data.\n",
    "   - Common kernel functions include:\n",
    "     - **Linear Kernel:** No transformation is applied; the original features are used.\n",
    "     - **Polynomial Kernel:** Maps the data into a higher-dimensional space using polynomial functions.\n",
    "     - **Radial Basis Function (RBF) Kernel (Gaussian Kernel):** Maps data into an infinite-dimensional space, often used for complex non-linear relationships.\n",
    "     - **Sigmoid Kernel:** Similar to a neural network activation function, used in certain types of SVM models.\n",
    "\n",
    "5. **Soft Margin:**\n",
    "   - In cases where the data is not linearly separable, SVM allows some misclassification by introducing a \"soft margin.\" This means that the algorithm finds a balance between maximizing the margin and minimizing the classification error.\n",
    "   - A regularization parameter, often denoted as \\(C\\), controls this trade-off. A small \\(C\\) value allows a wider margin with more misclassifications, while a large \\(C\\) value tries to classify all data points correctly, possibly at the cost of a smaller margin.\n",
    "\n",
    "### How SVM is Used in Machine Learning:\n",
    "\n",
    "1. **Classification:**\n",
    "   - SVM is primarily used for binary classification tasks. It can also be extended to multi-class classification using strategies like one-vs-one or one-vs-all.\n",
    "   - SVM is especially effective in cases where the number of features is greater than the number of samples, such as in text classification or bioinformatics.\n",
    "\n",
    "2. **Regression:**\n",
    "   - In Support Vector Regression (SVR), the goal is to find a function that deviates from the actual target values by a value no greater than a specified margin and is as flat as possible.\n",
    "\n",
    "3. **Outlier Detection:**\n",
    "   - SVM can be adapted for anomaly detection by finding the hyperplane that best separates the majority of the data from potential outliers.\n",
    "\n",
    "### Advantages of SVM:\n",
    "\n",
    "- **Effective in High-Dimensional Spaces:** SVM is well-suited for high-dimensional data where the number of features exceeds the number of samples.\n",
    "- **Robust to Overfitting:** Especially when using the soft margin approach, SVM can avoid overfitting by allowing some flexibility in classification.\n",
    "- **Versatility:** Through the use of different kernel functions, SVM can handle both linear and non-linear classification tasks.\n",
    "\n",
    "### Disadvantages of SVM:\n",
    "\n",
    "- **Computationally Intensive:** SVM can be slow to train, especially with large datasets.\n",
    "- **Less Effective on Noisy Data:** SVM can be sensitive to noise and outliers in the data, particularly when the margin is small.\n",
    "- **Choice of Kernel:** The performance of an SVM model heavily depends on the choice of the kernel and the tuning of hyperparameters, which can be challenging and time-consuming.\n",
    "\n",
    "### Example in Practice:\n",
    "\n",
    "Imagine you’re working on a binary classification problem, such as spam detection in emails. You have a dataset with numerous features, such as word frequencies, email metadata, and other characteristics. By applying SVM with an appropriate kernel (like the RBF kernel), you can train a model to classify new emails as spam or not spam. The SVM model will find the hyperplane that best separates spam emails from non-spam emails, aiming to generalize well to unseen data.\n",
    "\n",
    "In summary, SVM is a powerful and versatile tool in the machine learning toolbox, particularly useful for classification tasks in high-dimensional spaces and situations where a well-defined decision boundary is needed."
   ],
   "id": "d021597246671425"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "Cross-validation is a technique used in machine learning to assess how well a model will generalize to an independent dataset, that is, to test its performance on unseen data. It’s particularly useful for estimating the performance of a model when the available data is limited.\n",
    "\n",
    "### Key Concepts of Cross-Validation:\n",
    "\n",
    "1. **Overfitting and Underfitting:**\n",
    "   - **Overfitting** occurs when a model learns the training data too well, capturing noise and outliers, which results in poor performance on new data.\n",
    "   - **Underfitting** happens when a model is too simple and fails to capture the underlying patterns in the data.\n",
    "   - Cross-validation helps in balancing the trade-off between overfitting and underfitting by evaluating the model's performance on multiple subsets of the data.\n",
    "\n",
    "2. **Train-Test Split:**\n",
    "   - Before diving into cross-validation, it's important to understand the basic idea of splitting your dataset into two parts: the **training set** and the **test set**.\n",
    "   - The model is trained on the training set and then evaluated on the test set to estimate how it would perform on unseen data. However, this single split may not always be reliable, especially if the dataset is small or has inherent variability.\n",
    "\n",
    "3. **K-Fold Cross-Validation:**\n",
    "   - **K-Fold Cross-Validation** is the most commonly used form of cross-validation.\n",
    "   - The dataset is randomly divided into `k` equally (or almost equally) sized folds or subsets.\n",
    "   - The model is trained on `k-1` folds and tested on the remaining fold. This process is repeated `k` times, with each fold being used exactly once as the test set.\n",
    "   - The overall performance metric is then averaged over the `k` iterations to give a more reliable estimate of model performance.\n",
    "   - Common choices for `k` include 5 or 10, but this can vary depending on the size of the dataset.\n",
    "\n",
    "   Example of 5-Fold Cross-Validation:\n",
    "   - The dataset is split into 5 folds.\n",
    "   - Train on folds 1-4, test on fold 5.\n",
    "   - Train on folds 1-3 and 5, test on fold 4.\n",
    "   - Continue this process until each fold has been used as the test set once.\n",
    "   - Calculate the average of the performance metrics (e.g., accuracy, F1-score) from all 5 tests.\n",
    "\n",
    "4. **Leave-One-Out Cross-Validation (LOO-CV):**\n",
    "   - In **Leave-One-Out Cross-Validation**, `k` is set to the number of samples in the dataset (i.e., each test set contains only one observation).\n",
    "   - This method is computationally expensive for large datasets but provides a very thorough evaluation.\n",
    "\n",
    "5. **Stratified K-Fold Cross-Validation:**\n",
    "   - In **Stratified K-Fold Cross-Validation**, the folds are created in such a way that the proportion of classes (in the case of classification) is preserved in each fold.\n",
    "   - This is particularly important for imbalanced datasets where one class is much more frequent than others.\n",
    "\n",
    "6. **Hold-Out Method:**\n",
    "   - A simpler alternative to k-fold cross-validation is the **hold-out method**, where the dataset is randomly split into a training set and a test set (e.g., 70% for training and 30% for testing).\n",
    "   - This method is quicker but less reliable since the performance estimate is based on a single train-test split.\n",
    "\n",
    "### Why Cross-Validation is Important:\n",
    "\n",
    "- **Reliable Performance Estimation:** Cross-validation provides a more accurate estimate of model performance by reducing the variability that might come from a single train-test split.\n",
    "- **Hyperparameter Tuning:** It’s commonly used in conjunction with hyperparameter tuning methods like grid search or random search to find the best set of parameters for a model.\n",
    "- **Model Selection:** Helps in comparing different models or algorithms by providing a consistent metric across different evaluations.\n",
    "- **Preventing Overfitting:** By testing the model on multiple different subsets of the data, cross-validation helps to ensure that the model is not simply memorizing the training data but is learning to generalize.\n",
    "\n",
    "### Limitations of Cross-Validation:\n",
    "\n",
    "- **Computational Cost:** For large datasets or complex models, cross-validation can be computationally expensive, especially as `k` increases.\n",
    "- **Data Leakage Risk:** If data preprocessing steps (like scaling or feature selection) are performed before splitting the data into folds, it can lead to data leakage, where information from the test set unintentionally influences the model. To avoid this, preprocessing should be done within each fold separately.\n",
    "\n",
    "### Example in Practice:\n",
    "\n",
    "Suppose you’re building a model to predict whether a customer will churn based on various features. Instead of relying on a single train-test split, you can apply 5-fold cross-validation. This way, you’ll get a more robust estimate of how well your model is likely to perform on new customer data, leading to more reliable decision-making when deploying the model in a production environment.\n",
    "\n",
    "In summary, cross-validation is a crucial technique in machine learning for validating the performance of models, ensuring that they generalize well to unseen data and helping prevent overfitting."
   ],
   "id": "dc8c1099a6e2e432"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "Entropy in machine learning is a concept borrowed from information theory, where it measures the amount of uncertainty or impurity in a dataset. In the context of machine learning, particularly in decision tree algorithms like ID3, C4.5, and CART, entropy is used to quantify the randomness or unpredictability of the information being processed, and it helps in determining the best feature to split the data at each step of building the tree.\n",
    "\n",
    "### Key Concepts of Entropy:\n",
    "\n",
    "1. **Information Theory:**\n",
    "   - Entropy is a measure of the unpredictability or information content of a random variable. It was introduced by Claude Shannon in his seminal work on information theory.\n",
    "   - In simpler terms, it quantifies how much \"disorder\" or \"uncertainty\" there is in a set of outcomes.\n",
    "\n",
    "2. **Mathematical Definition:**\n",
    "   - For a binary classification problem, if you have a dataset with two classes (say, positive and negative examples), the entropy \\( H \\) of the dataset can be calculated as:\n",
    "\n",
    "   $$\n",
    "   H(S) = -p_1 \\log_2(p_1) - p_2 \\log_2(p_2)\n",
    "   $$\n",
    "\n",
    "   - Here, \\( p_1 \\) and \\( p_2 \\) represent the proportions of positive and negative examples in the dataset, respectively.\n",
    "   - If the dataset is perfectly mixed (i.e., 50% positive and 50% negative), the entropy will be 1, indicating maximum disorder.\n",
    "   - If the dataset is perfectly homogeneous (i.e., 100% of one class), the entropy will be 0, indicating no disorder.\n",
    "\n",
    "3. **Entropy in Decision Trees:**\n",
    "   - In decision tree algorithms, entropy is used to decide how to split the data at each node.\n",
    "   - The algorithm chooses the feature that minimizes the entropy (or maximizes the information gain) after the split. This process helps in making the tree as simple and as generalizable as possible.\n",
    "\n",
    "4. **Information Gain:**\n",
    "   - **Information Gain (IG)** is the reduction in entropy after a dataset is split on an attribute. It measures how much uncertainty is reduced by splitting the data according to a particular feature.\n",
    "   - The information gain is calculated as:\n",
    "\n",
    "   $$\n",
    "   IG(S, A) = H(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} H(S_v)\n",
    "   $$\n",
    "\n",
    "   - Where:\n",
    "     - \\( H(S) \\) is the entropy of the original set.\n",
    "     - \\( S_v \\) is the subset of \\( S \\) for which feature \\( A \\) has value \\( v \\).\n",
    "     - The sum is over all possible values of the feature \\( A \\).\n",
    "\n",
    "   - A feature with a higher information gain is considered better for splitting the data.\n",
    "\n",
    "### Example of Entropy in a Decision Tree:\n",
    "\n",
    "Suppose you're building a decision tree to classify whether an email is spam or not. If you start with a dataset where half the emails are spam and half are not, the initial entropy is 1 (maximum uncertainty). If you split the emails based on a feature like \"contains the word 'offer',\" and this results in one group of emails being 80% spam and another being 20% spam, the entropy after the split will be lower. The decision tree algorithm will favor this feature for splitting because it significantly reduces the uncertainty (entropy) of the classification.\n",
    "\n",
    "### Intuition Behind Entropy:\n",
    "\n",
    "- **Low Entropy (Close to 0):** Indicates a group of items that are mostly of one class (low impurity). For example, if 99% of the data points belong to one class, the entropy is very low, close to 0.\n",
    "- **High Entropy (Close to 1):** Indicates a group of items that are evenly split between different classes (high impurity). For example, if the data is evenly split between two classes, the entropy is at its maximum, which is 1 for binary classification.\n",
    "\n",
    "### Importance of Entropy in Machine Learning:\n",
    "\n",
    "- **Feature Selection in Decision Trees:** Entropy helps in selecting the best features to split the data, which is crucial in building efficient decision trees.\n",
    "- **Understanding Uncertainty:** By calculating entropy, machine learning algorithms can quantify and reduce uncertainty, leading to more accurate and reliable models.\n",
    "- **Model Interpretability:** Entropy-based models like decision trees are often more interpretable than other complex models, providing insights into how decisions are made.\n",
    "\n",
    "### Limitations of Entropy:\n",
    "\n",
    "- **Computational Cost:** Calculating entropy for large datasets or for many features can be computationally expensive.\n",
    "- **Bias Towards Features with More Levels:** Entropy-based methods may favor features with more levels (more possible values), which can lead to overfitting.\n",
    "\n",
    "In summary, entropy is a measure of uncertainty or impurity in a dataset and plays a crucial role in decision tree algorithms for feature selection and data splitting. It is a fundamental concept in machine learning that helps in building models that are both accurate and interpretable."
   ],
   "id": "8a90757bc88373b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "In machine learning, an **epoch** refers to one complete pass through the entire training dataset by the learning algorithm. It is a critical concept in the training process of models, especially in neural networks.\n",
    "\n",
    "### Key Concepts of an Epoch:\n",
    "\n",
    "1. **Training Process:**\n",
    "   - When training a machine learning model, especially deep learning models like neural networks, the dataset is typically too large to be fed into the model all at once. Instead, the data is divided into smaller batches.\n",
    "   - During training, the model updates its weights after each batch. An epoch is completed once the model has iterated over every batch of the entire dataset.\n",
    "\n",
    "2. **Multiple Epochs:**\n",
    "   - Training a model usually involves running through the dataset multiple times, meaning multiple epochs are performed. This allows the model to continue learning and refining its parameters (such as weights in a neural network).\n",
    "   - With each epoch, the model ideally gets better at predicting the outcomes as it learns from the data repeatedly.\n",
    "\n",
    "3. **Convergence:**\n",
    "   - The goal of training over multiple epochs is to minimize the model's loss function, which measures how well the model's predictions match the actual targets. As the number of epochs increases, the model should converge to an optimal set of parameters.\n",
    "   - However, too many epochs can lead to overfitting, where the model performs very well on the training data but poorly on unseen test data.\n",
    "\n",
    "4. **Epoch vs. Batch vs. Iteration:**\n",
    "   - **Batch:** A batch is a subset of the training data. Instead of updating the model after each individual training example, the model is updated after each batch of examples.\n",
    "   - **Iteration:** An iteration refers to a single update of the model's parameters. The number of iterations in an epoch equals the number of batches.\n",
    "   - **Epoch:** As mentioned, an epoch is one complete pass through the entire training dataset.\n",
    "\n",
    "   For example, if your dataset has 10,000 samples and your batch size is 100, it will take 100 iterations to complete one epoch.\n",
    "\n",
    "5. **Impact of Epochs on Training:**\n",
    "   - **Too Few Epochs:** The model may underfit, meaning it hasn't learned enough from the data.\n",
    "   - **Optimal Number of Epochs:** The model achieves good generalization, meaning it performs well on both the training data and unseen data.\n",
    "   - **Too Many Epochs:** The model may overfit, meaning it performs very well on the training data but poorly on unseen data due to learning the noise in the training data.\n",
    "\n",
    "### Example in Practice:\n",
    "\n",
    "Consider training a neural network to recognize handwritten digits using the MNIST dataset. If you train the model for one epoch, the model will see each of the 60,000 training images once and then update its parameters based on the errors it made. If you train for 10 epochs, the model will see each image 10 times, potentially leading to better performance as it continues to learn from the data.\n",
    "\n",
    "### Monitoring Epochs:\n",
    "\n",
    "During training, it's common to monitor the model's performance after each epoch by evaluating it on a validation set. This helps in determining the optimal number of epochs. Techniques like **early stopping** are used to stop the training process when the performance on the validation set stops improving, even if the model hasn't yet completed the planned number of epochs.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "In summary, an epoch is a full pass through the entire training dataset and is a fundamental concept in the training process of machine learning models. The number of epochs is a hyperparameter that needs to be carefully chosen to ensure that the model generalizes well to new data."
   ],
   "id": "e5726b2bbe50432b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8786beff6f5203ee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
